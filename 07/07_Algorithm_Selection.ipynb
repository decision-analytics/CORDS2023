{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/logo_wiwi_en_.png\" width=\"30%\" align=\"left\">\n",
    "\n",
    "<img src=\"img/decision_analytics_logo.png\" width=\"17%\" align=\"right\">\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "# Combining OR and Data Science\n",
    "\n",
    "**Summer Term 2023**\n",
    "\n",
    "\n",
    "# 7. Algorithm Selection\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**J-Prof. Dr. Michael Römer, Till Porrmann, Mohsen Nafar**\n",
    "\n",
    "**Decision Analytics Group  | Bielefeld University**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "/* Any CSS style can go in here. */\n",
       ".dataframe th {\n",
       "    font-size: 25px;\n",
       "}\n",
       ".dataframe td {\n",
       "    font-size: 25px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* Any CSS style can go in here. */\n",
    ".dataframe th {\n",
    "    font-size: 25px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 25px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining OR and Data Science: The Two Parts of the Course\n",
    "\n",
    "In **part I**, we dealt with **combining OR and DS for decision-making under uncertainty**:\n",
    "- using DS for supporting **modeling** uncertainty in OR approaches (e.g. stochastic programming)\n",
    "\n",
    "In **part II**, we dealt with **combining OR and DS for selecting and configuring (OR) algorithms**:\n",
    "- using DS for supporting / improving the **solution** process of OR models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part II: Algorithm Selection and Configuration\n",
    "\n",
    "For many complex combinatorial optimization problems, we have\n",
    "- many different algorithms that can be used to solve them and\n",
    "- these algorithms exhibit many configurable parameters\n",
    "\n",
    "In general,\n",
    "- there is not a single algorithm that work best for each problem (instance)\n",
    "- for a given algorithm, there is not a single parameter configuration that is best for each problem (instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this part of the course, we learn how to use Machine Learning approaches for\n",
    "- selecting a good algorithm \n",
    "- finding a good parameter configuration\n",
    "\n",
    "for a given problem (instance) based on problem / instance features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Week: Algorithm Selection\n",
    "\n",
    "\n",
    "**Introduction to Algorithm Selection**\n",
    "- Introduction and motivation\n",
    "- Introducing a case study from a former algorithm selection competition\n",
    "- Analyzing the data and motivating instance-specific algorithm selection\n",
    "\n",
    "**Algorithm Selection using Unsupervised Machine Learning**\n",
    "- Using features for algorithm selection\n",
    "- Short review: Unsupervised learning\n",
    "- Cluster-and-select: Using unsupervised learning for feature-based algorithm selection\n",
    "\n",
    "**Evaluating Algorithm Selection Approaches Using Cross Validation**\n",
    "\n",
    "\n",
    "**Algorithm Selection using Classification: Predicting the Best Approach**\n",
    "- brief review of ML approaches for classification\n",
    "- classification for algorithm selection\n",
    "\n",
    "**Algorithm Selection using Regression: Predicting the Algorithm Performance**\n",
    "- brief review of ML approaches for regression\n",
    "- predicting algorithm runtime and performance using regression\n",
    "- selecting algorithms based on these predicitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithm Selection: The Task\n",
    "\n",
    "\n",
    "![algo](img/algoselection_scheme.png)\n",
    "\n",
    "Given a set of algorithms $a_1, \\ldots a_n$ for a hard opimization problem, which one will solve a given problem instance, e.g. $i_3$, the fastest?\n",
    "\n",
    "**In general, there is no free lunch:**\n",
    "- there is no algorithm dominating all others, but:\n",
    "- for different problems and different problem instances, different algorithms may be best\n",
    "\n",
    "...and the difference between the runtime of the algorithms is often practically significant and sometimes huge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing our Algorithm Selection Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case Study Example: MaxSat Data Set\n",
    "\n",
    "**Algorithm Selection Library (ASLib)**\n",
    "- a library of data sets  for algorithm selection research\n",
    "- data sets for different problem classes, each set includes:\n",
    "  - instance information, in particular instance features\n",
    "  - runtime information for each instance and various algorithms\n",
    "- we will use this data set to illustrate how to select the best algorithm!\n",
    "\n",
    "..we somewhat pretend that we have to solve similar problems in the future and that we can use the given data to \"train\" our selection approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**MaxSAT data set from ASLib**\n",
    "- MaxSAT (Maximum Satisfiability) is a problem class in computer\n",
    "  science that can be used e.g. for configuration problems\n",
    "  - there are various algorithms for the MaxSAT problem\n",
    "- our data set has **601 instances** each of which was solved with **19 different algorithms**\n",
    "- **algorithm performance** is measured using the **PAR10 score**:\n",
    "  - each algorithm is run at most **1800  seconds**\n",
    "  - in case of **timeout**, the PAR10 score is **10 * 1800 seconds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A First Look at the Data\n",
    "\n",
    "We first read the runtime data (that is the PAR10 score for each combination of instance and algorithm) from a csv file and give the columns the appropriate names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#https://raw.githubusercontent.com/decision-analytics/cords2022/main/07/feature_values.csv\n",
    "df_runs = pd.read_csv(\"https://raw.githubusercontent.com/decision-analytics/cords2022/main/07/algorithm_runs.csv\", header = None)\n",
    "df_runs.columns = ['instance_id', 'repetition', 'algorithm', 'PAR10', 'runstatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We then drop two columns that are not needed for our purpose. This means that the remaining columns contain:\n",
    "- the instance id\n",
    "- the the name of the algorithm\n",
    "- the PAR10 score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>PAR10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>CCEHC2akms</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>ahms-1.70</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>LMHS-2016</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>Optiriss6</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>WPM3-2015-co</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>TWComp_win95pts_N76.wcnf</td>\n",
       "      <td>WMaxSatz+</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11415</th>\n",
       "      <td>TWComp_win95pts_N76.wcnf</td>\n",
       "      <td>CCLS2akms</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11416</th>\n",
       "      <td>TWComp_win95pts_N76.wcnf</td>\n",
       "      <td>QMaxSAT14</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11417</th>\n",
       "      <td>TWComp_win95pts_N76.wcnf</td>\n",
       "      <td>ahms-ls-1.70</td>\n",
       "      <td>18000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11418</th>\n",
       "      <td>TWComp_win95pts_N76.wcnf</td>\n",
       "      <td>Open-WBO15</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11419 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    instance_id     algorithm     PAR10\n",
       "0                 mul_8_11.wcnf    CCEHC2akms  18000.00\n",
       "1                 mul_8_11.wcnf     ahms-1.70  18000.00\n",
       "2                 mul_8_11.wcnf     LMHS-2016  18000.00\n",
       "3                 mul_8_11.wcnf     Optiriss6  18000.00\n",
       "4                 mul_8_11.wcnf  WPM3-2015-co  18000.00\n",
       "...                         ...           ...       ...\n",
       "11414  TWComp_win95pts_N76.wcnf     WMaxSatz+  18000.00\n",
       "11415  TWComp_win95pts_N76.wcnf     CCLS2akms  18000.00\n",
       "11416  TWComp_win95pts_N76.wcnf     QMaxSAT14      1.63\n",
       "11417  TWComp_win95pts_N76.wcnf  ahms-ls-1.70  18000.00\n",
       "11418  TWComp_win95pts_N76.wcnf    Open-WBO15      1.41\n",
       "\n",
       "[11419 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_runs.drop(columns=['repetition', 'runstatus'], inplace=True, errors='ignore') # inplace=True means that the data frame itself is changed \n",
    "df_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspecting the MaxSat Data Set\n",
    "\n",
    "- we now extract the instances and the algorithms from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We have 601 instances and 19 algorithms\n"
     ]
    }
   ],
   "source": [
    "instances = df_runs[\"instance_id\"].unique()\n",
    "algorithms = df_runs[\"algorithm\"].unique()\n",
    "print(f\" We have {len(instances)} instances and {len(algorithms)} algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- let us compute a data frame with the best algorithm run per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>PAR10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8179</th>\n",
       "      <td>10tree110p.wcnf</td>\n",
       "      <td>mscg2015a</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8654</th>\n",
       "      <td>10tree115p.wcnf</td>\n",
       "      <td>mscg2015a</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8198</th>\n",
       "      <td>10tree120p.wcnf</td>\n",
       "      <td>mscg2015a</td>\n",
       "      <td>4.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>10tree125p.wcnf</td>\n",
       "      <td>mscg2015b</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216</th>\n",
       "      <td>10tree130p.wcnf</td>\n",
       "      <td>mscg2015b</td>\n",
       "      <td>25.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          instance_id  algorithm  PAR10\n",
       "8179  10tree110p.wcnf  mscg2015a   1.37\n",
       "8654  10tree115p.wcnf  mscg2015a   0.90\n",
       "8198  10tree120p.wcnf  mscg2015a   4.88\n",
       "8672  10tree125p.wcnf  mscg2015b   3.20\n",
       "8216  10tree130p.wcnf  mscg2015b  25.09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_row_idx = df_runs.groupby(\"instance_id\")[\"PAR10\"].idxmin()  # We group all entries with the same instance_id and use [\"PAR10\"].idxmin() to return the row id with the lowest PAR10 value\n",
    "df_best_runs = df_runs.iloc[min_row_idx]  # We only select those rows that contain the minimum PAR10 value for each instance (iloc takes the row id as input)\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How many algorithms performed best on at least one instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_best_runs[\"algorithm\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up a \"Virtual\" Experiment: Splitting the Data\n",
    "To simulate an algorithm selection experiment, we split the data as follows:\n",
    "- 2/3 of the instances (and their algorithm runs) are used as \"training data\" used for training a selection approach / policy\n",
    "- the remaining 1/3 of the instances are used for evaluation purposes, that is to see how good the selection approach works on \"unknown\" instances\n",
    "- we can use the `train_test_split` function from scikit-learn to split the set of instances accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_instances, test_instances = train_test_split(instances, test_size=0.33, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- now, we use these intance sets to split the two dataframes containing the runtime data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_train_test(df_data, train_instances, test_instances):\n",
    "    return df_data[df_data[\"instance_id\"].isin(train_instances)], df_data[df_data[\"instance_id\"].isin(test_instances)]\n",
    "\n",
    "df_runs_train, df_runs_test =  get_data_train_test(df_runs, train_instances, test_instances)\n",
    "df_best_runs_train, df_best_runs_test =  get_data_train_test(df_best_runs, train_instances, test_instances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting a Single \"Best\" Algorithm for All Instances?\n",
    "\n",
    "Let us start simple: What if we were only allowed to choose a single algorithm that we're going to apply to the test set.\n",
    "\n",
    "We will consider two variants:\n",
    "- selecting the algorithm that was the best most often in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mscg2015a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_algorithm_with_most_best_results(df_best_runs_train):\n",
    "    \n",
    "    df_number_times_best = df_best_runs_train.groupby(\"algorithm\")[\"instance_id\"].size().reset_index()\n",
    "\n",
    "    return df_number_times_best.iloc[df_number_times_best[\"instance_id\"].idxmax()][\"algorithm\"]\n",
    "\n",
    "get_algorithm_with_most_best_results(df_best_runs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- selecting the algorithm that with the best average performance in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WPM3-2015-co'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_algorithm_with_best_average_performance(df_runs_train):\n",
    "\n",
    "    average_par10_train = df_runs_train.groupby(\"algorithm\")[\"PAR10\"].mean().reset_index()\n",
    "\n",
    "    min_row_idx = average_par10_train[\"PAR10\"].idxmin()  # We use [\"PAR10\"].idxmin() to return the row id with the lowest PAR10 value\n",
    "    return average_par10_train.iloc[min_row_idx]['algorithm']  \n",
    "\n",
    "get_algorithm_with_best_average_performance(df_runs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting a Single \"Best\" Algorithm: Results of the Evaluation\n",
    "\n",
    "To see which approach performs better, let us compute the **average performance** on the **test instances**:\n",
    "\n",
    "*Note: In a realistic setting, we would perform the evaluation runs with unknown instances. Here, we have all the runtime data available from \"historical\" runs*\n",
    "\n",
    "- performance of the most-often-best from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2953.7805025125626"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_runs_test_selected_algorithm_most_best = df_runs_test[df_runs_test['algorithm']==get_algorithm_with_most_best_results(df_best_runs_train)]\n",
    "\n",
    "df_runs_test_selected_algorithm_most_best['PAR10'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- selecting the algorithm with the best average performance on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2177.582462311558"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_runs_test_selected_algorithm_best_avg_performance = df_runs_test[df_runs_test['algorithm']==get_algorithm_with_best_average_performance(df_runs_train)]\n",
    "\n",
    "df_runs_test_selected_algorithm_best_avg_performance['PAR10'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting the  Results into Perspective: Two Benchmark Values\n",
    "\n",
    "To see how good our selection was, let us consider two bounds / benchmark values:\n",
    "\n",
    "- the average performance of all algorithms on the test set (we should be better than that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7274.185416556466"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_average_test = df_runs_test[\"PAR10\"].mean()\n",
    "performance_average_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the average performance if we would select the best algorithm for each instance in the test set (we assume that we have an \"oracle\" telling us the best approach) (we cannot do better than that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400.7546231155777"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_oracle_test = df_best_runs_test[\"PAR10\"].mean()\n",
    "performance_oracle_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collecting all Results in One Data Frame\n",
    "- Let us now collect all results so far in a single data frame\n",
    "- we will continue growing this data frame during this meeting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test\n",
       "average_performance_test_set  7274.185417\n",
       "oracle                        1400.754623\n",
       "single_best(best_avg)         2177.582462\n",
       "single_best(most_often_best)  2953.780503"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(columns=['perf_test'])\n",
    "df_results.loc['average_performance_test_set'] = performance_average_test\n",
    "df_results.loc['oracle'] = performance_oracle_test\n",
    "df_results.loc['single_best(best_avg)'] = df_runs_test_selected_algorithm_best_avg_performance['PAR10'].mean()\n",
    "df_results.loc['single_best(most_often_best)'] = df_runs_test_selected_algorithm_most_best['PAR10'].mean()\n",
    "\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:** *These results depend on the random train-test split of the instances. To reproduce the results, use the same random_state as used for the split in this notebook!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Exercises:\n",
    "\n",
    "- See whether changing the train-test-split (by changing the parameter \"random_state\" in the train-test split function) affects the selection of the best algorithms as well as the overall results (the original value in the uploaded notebook was 11)\n",
    "- Which algorithm is most often best for the test instances? Is it the same as the one being best most often for the training set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instance Features for Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Idea: Use Instance Features for Selecting the Best Algorithm\n",
    "\n",
    "Can't we do better? Yes, by making a \"tailored\" selection that considers instance properties!\n",
    "\n",
    "**Assumption:**\n",
    "\n",
    "Instances \"prefer\" different solvers/algorithms due to the variations in their structure\n",
    "    \n",
    "    \n",
    "\n",
    "**Idea** Represent the structure of instances through a **feature vector** that we can use for **machine learning**\n",
    "\n",
    "**Goal:** Use Machine Learning to support the selection of algorithms based on the instance features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples for Instance Features: Traveling Salesperson Problem\n",
    "\n",
    "Consider the **Traveling Salesperson Problem** (TSP):\n",
    "    \n",
    "**Example Features:**\n",
    "- Number of nodes\n",
    "- Statistics about the distance matrix (mean, std. dev., etc.)\n",
    "- Features describing the node distribution\n",
    "    - Statistics about the node degree of the spanning tree (incoming, outgoing)\n",
    "    - ..\n",
    "    \n",
    "\n",
    "Pihera, J., & Musliu, N. (2014). Application of Machine Learning to Algorithm Selection for TSP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features for Broader Problem Classes\n",
    "\n",
    "Consider a broader problem class: **Mixed Integer Programming** (MIP)\n",
    "\n",
    "**Example features:**\n",
    "\n",
    "- Number of variables, Number of constraints\n",
    "- Number of nonzeros\n",
    "- Percentage of integer, binary and continuous variables\n",
    "- Number and percentage of certaint constraint types\n",
    "- Features from the starting of the solution process, e.g. cuts of certain types\n",
    "- ..\n",
    "\n",
    "Georges, A. et al. (2018): Feature-Based Algorithm Selection for Mixed Integer Programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instance Features for the MaxSAT Data Set\n",
    "\n",
    "- the MaxSAT data set comes with a file with 37 features per instance\n",
    "- the semantics of the features are unknown to us (there names are f_1, f_2, etc.)\n",
    "- the values are normalized to take values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>f_31</th>\n",
       "      <th>f_32</th>\n",
       "      <th>f_33</th>\n",
       "      <th>f_34</th>\n",
       "      <th>f_35</th>\n",
       "      <th>f_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnf.13.p.9.wcnf</td>\n",
       "      <td>87124.0</td>\n",
       "      <td>393887.0</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22119</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03386</td>\n",
       "      <td>0.78190</td>\n",
       "      <td>0.95207</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.78190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnf.19.p.10.wcnf</td>\n",
       "      <td>132880.0</td>\n",
       "      <td>600905.0</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22113</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03374</td>\n",
       "      <td>0.78194</td>\n",
       "      <td>0.95207</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.78194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnf.12.t.9.wcnf</td>\n",
       "      <td>74960.0</td>\n",
       "      <td>341160.0</td>\n",
       "      <td>0.01219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.21972</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03670</td>\n",
       "      <td>0.78051</td>\n",
       "      <td>0.95123</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.78051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnf.8.p.9.wcnf</td>\n",
       "      <td>51700.0</td>\n",
       "      <td>233615.0</td>\n",
       "      <td>0.01199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22130</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03404</td>\n",
       "      <td>0.78189</td>\n",
       "      <td>0.95206</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.78189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnf.14.p.9.wcnf</td>\n",
       "      <td>88600.0</td>\n",
       "      <td>400577.0</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22118</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03386</td>\n",
       "      <td>0.78191</td>\n",
       "      <td>0.95207</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.78191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        instance_id       f_0       f_1      f_2  f_3  f_4  f_5  f_6      f_7  \\\n",
       "0   cnf.13.p.9.wcnf   87124.0  393887.0  0.01198  1.0  0.0  1.0  1.0  0.22119   \n",
       "1  cnf.19.p.10.wcnf  132880.0  600905.0  0.01198  1.0  0.0  1.0  1.0  0.22113   \n",
       "2   cnf.12.t.9.wcnf   74960.0  341160.0  0.01219  1.0  0.0  1.0  1.0  0.21972   \n",
       "3    cnf.8.p.9.wcnf   51700.0  233615.0  0.01199  1.0  0.0  1.0  1.0  0.22130   \n",
       "4   cnf.14.p.9.wcnf   88600.0  400577.0  0.01198  1.0  0.0  1.0  1.0  0.22118   \n",
       "\n",
       "       f_8  ...  f_27     f_28     f_29     f_30     f_31  f_32     f_33  \\\n",
       "0  0.00003  ...   1.0  0.03386  0.78190  0.95207  0.00002   0.0  0.00001   \n",
       "1  0.00002  ...   1.0  0.03374  0.78194  0.95207  0.00001   0.0  0.00000   \n",
       "2  0.00003  ...   1.0  0.03670  0.78051  0.95123  0.00002   0.0  0.00001   \n",
       "3  0.00004  ...   1.0  0.03404  0.78189  0.95206  0.00003   0.0  0.00001   \n",
       "4  0.00003  ...   1.0  0.03386  0.78191  0.95207  0.00002   0.0  0.00000   \n",
       "\n",
       "      f_34     f_35     f_36  \n",
       "0  0.00007  0.00006  0.78190  \n",
       "1  0.00004  0.00004  0.78194  \n",
       "2  0.00008  0.00007  0.78051  \n",
       "3  0.00011  0.00010  0.78189  \n",
       "4  0.00006  0.00006  0.78191  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_instance_features= pd.read_csv(\"https://raw.githubusercontent.com/decision-analytics/cords2022/main/07/feature_values.csv\", header = None)\n",
    "df_instance_features.columns = ['instance_id', 'repetition'] + [f'f_{i}' for i in range(df_instance_features.shape[1] - 2)]\n",
    "df_instance_features.drop(columns='repetition',inplace=True,errors='ignore')\n",
    "df_instance_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also split the features data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_instance_features_train, df_instance_features_test = get_data_train_test(df_instance_features, train_instances, test_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Clustering Instances for Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Unsupervised) Clustering for Algorithm Selection\n",
    "\n",
    "**Question:** How can we use  feature information to support the (instance-specific) selection of algorithms?\n",
    "\n",
    "\n",
    "**A first  idea: Cluster-and-Select**\n",
    "\n",
    "- **Cluster** similar instances in the training data based on feature information\n",
    "- **Select** the best algorithm, for each cluster e.g. the one with lowest average PAR10 value in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**For an \"unseen\" instance**, we can then:\n",
    "- determine the cluster it belongs to\n",
    "- choose the algorithm for that was selected for the cluster in the training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning: Unsupervised vs Supervised Learning\n",
    "\n",
    "**Unsupervised Learning**\n",
    "- given a vector $\\boldsymbol{x}_i$ input data features for each observation / instance $i$\n",
    "- learn something interesting such as relations or clusters \n",
    "\n",
    "**Supervised Learning:**\n",
    "- given a vector $\\boldsymbol{x}_i$  input data features\n",
    "- and a label $y_i$ for each $i$\n",
    "- learn the to predict $y_j$ for an unlabeled feature vector $\\boldsymbol{x}_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Does Clustering Work?\n",
    "\n",
    "\n",
    "**Basic Idea**\n",
    "\n",
    "- Group data points that are \"similar\"\n",
    "\n",
    "\n",
    "Raw Data Points | Clustered Data Points\n",
    "- | - \n",
    "![clus](img/clusters.png) | ![clus1](img/clusters_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Does Clustering Work?: $k$-Means Clustering\n",
    "\n",
    "$k$-Means clustering is one of the best-known clustering approaches\n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "- Form $k$ **clusters** using some distance metric, for example the Euclidean distance\n",
    "- Find an optimal set of $k$ cluster **centers**\n",
    "\n",
    "Cluster Centers (green) | Cluster Regions Induced by tje Centers (for new data points)\n",
    "- | - \n",
    " ![clus1](img/clusters_2.png) | ![clus2](img/clusters_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$ -Means Optimization Problem\n",
    "\n",
    "The problem of finding the best $k$ cluster centers can be stated as an  optimization problem:\n",
    "    \\begin{equation*}\n",
    "        \\underset{\\mathbf{S}} {\\operatorname{arg\\,min}}  \\sum_{i=1}^{k} \\sum_{\\mathbf x \\in S_i} \\left\\| \\mathbf x - \\mu_i \\right\\|^2\n",
    "    \\end{equation*}\n",
    "\n",
    "where:\n",
    "- $\\mathbf{S}$ is a vector of  $k$ sets $S_1,\\dots,S_k$\n",
    "- $\\mathbf{x}$ is the input data that is clustered into sets $S_1,\\dots,S_k$\n",
    "- $\\mu_i$ is the mean of the points in $S_i$\n",
    "\n",
    "Unfortunately, this problem is very difficult to solve to optimality.\n",
    "\n",
    "- ML software usually uses iterative heuristic algorithms (not discussed here)\n",
    "- To predict the cluster of a new / unknown data point, we simply pick the cluster with the closest distance from the cluster mean to the new data point\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Unsupervised) Clustering for Algorithm Selection\n",
    "\n",
    "\n",
    "Let us see clustering in action for the **cluster-and-select** approach sketched above.\n",
    "\n",
    "Step 1: **Cluster** similar instances in the training data based on feature information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "clu = KMeans(n_clusters=10) # the clustering algorithm / model that we will use. Here, you may also take another one.\n",
    "\n",
    "prd = clu.fit_predict(df_instance_features_train.loc[:, \"f_0\":])  # Create clusters based on training instances and predict the cluster for each of the training instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnf.19.p.10.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnf.14.p.9.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cnf.20.p.10.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnf.15.p.9.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cnf.11.p.9.wcnf</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         instance_id  cluster\n",
       "1   cnf.19.p.10.wcnf        6\n",
       "4    cnf.14.p.9.wcnf        6\n",
       "5   cnf.20.p.10.wcnf        6\n",
       "8    cnf.15.p.9.wcnf        6\n",
       "12   cnf.11.p.9.wcnf        8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clustered_instances_train = df_instance_features_train.assign(cluster=prd)[['instance_id','cluster']]  # create a new datafram that contains the instances and their assigned cluster as columns\n",
    "\n",
    "df_clustered_instances_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Unsupervised) Clustering for Algorithm Selection\n",
    "\n",
    "Step 2: **Select** the best algorithm, for each cluster e.g.the one with lowest average PAR10 value in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## add the cluster to the runs data set\n",
    "df_runs_train_with_clusters = pd.merge(df_runs_train, df_clustered_instances_train, left_on='instance_id', right_on='instance_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "##compute the average performance of each algorithm per cluster\n",
    "df_average_performance_clusters = df_runs_train_with_clusters.groupby(['cluster','algorithm'])[\"PAR10\"].mean().reset_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>maxhs-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Open-WBO15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>mscg2015a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3</td>\n",
       "      <td>maxino16-c10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4</td>\n",
       "      <td>Open-WBO15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5</td>\n",
       "      <td>Optiriss6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6</td>\n",
       "      <td>mscg2015b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>7</td>\n",
       "      <td>maxino16-dis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>8</td>\n",
       "      <td>WPM3-2015-co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9</td>\n",
       "      <td>QMaxSAT16UC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cluster     algorithm\n",
       "14         0       maxhs-b\n",
       "23         1    Open-WBO15\n",
       "55         2     mscg2015a\n",
       "72         3  maxino16-c10\n",
       "80         4    Open-WBO15\n",
       "101        5     Optiriss6\n",
       "132        6     mscg2015b\n",
       "149        7  maxino16-dis\n",
       "163        8  WPM3-2015-co\n",
       "179        9   QMaxSAT16UC"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##select the algorithm with the best average performance\n",
    "df_algorithm_for_cluster_best_avg_performance = df_average_performance_clusters.iloc[df_average_performance_clusters.groupby('cluster')[\"PAR10\"].idxmin()] \n",
    "\n",
    "## only use the two colums \n",
    "df_algorithm_for_cluster_best_avg_performance = df_algorithm_for_cluster_best_avg_performance[['cluster','algorithm']]\n",
    "df_algorithm_for_cluster_best_avg_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Unsupervised) Clustering for Algorithm Selection\n",
    "\n",
    "**For an \"unseen\" instance**, we can then:\n",
    "- determine the cluster it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnf.13.p.9.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnf.12.t.9.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnf.8.p.9.wcnf</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cnf.13.p.8.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cnf.17.d.10.wcnf</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        instance_id  cluster\n",
       "0   cnf.13.p.9.wcnf        6\n",
       "2   cnf.12.t.9.wcnf        6\n",
       "3    cnf.8.p.9.wcnf        8\n",
       "6   cnf.13.p.8.wcnf        6\n",
       "7  cnf.17.d.10.wcnf        6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the clusters for the test instances\n",
    "prd = clu.predict(df_instance_features_test.loc[:, \"f_0\":])  \n",
    "\n",
    "# Add the assigned cluster to df_instances as a new column\n",
    "df_clustered_instances_test = df_instance_features_test.assign(cluster=prd)[['instance_id','cluster']] \n",
    "\n",
    "df_clustered_instances_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... and choose the best algorithm for that was selected for the cluster in the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnf.13.p.9.wcnf</td>\n",
       "      <td>6</td>\n",
       "      <td>mscg2015b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnf.12.t.9.wcnf</td>\n",
       "      <td>6</td>\n",
       "      <td>mscg2015b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnf.13.p.8.wcnf</td>\n",
       "      <td>6</td>\n",
       "      <td>mscg2015b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnf.17.d.10.wcnf</td>\n",
       "      <td>6</td>\n",
       "      <td>mscg2015b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnf.20.d.9.wcnf</td>\n",
       "      <td>6</td>\n",
       "      <td>mscg2015b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        instance_id  cluster  algorithm\n",
       "0   cnf.13.p.9.wcnf        6  mscg2015b\n",
       "1   cnf.12.t.9.wcnf        6  mscg2015b\n",
       "2   cnf.13.p.8.wcnf        6  mscg2015b\n",
       "3  cnf.17.d.10.wcnf        6  mscg2015b\n",
       "4   cnf.20.d.9.wcnf        6  mscg2015b"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_algorithm_test_instances = pd.merge(df_clustered_instances_test, df_algorithm_for_cluster_best_avg_performance, left_on='cluster', right_on='cluster')\n",
    "\n",
    "df_selected_algorithm_test_instances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Well Does this Selection Perform?\n",
    "\n",
    "Let us now evaluate this clustering-based selection on the test data set:\n",
    "- (we write a little helper function that we can re-use later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995.3791457286434"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_selected_algorithms(df_runs_test, df_selected_algorithms):       \n",
    "    df_runs_with_selected_algorithms = pd.merge(df_runs_test, df_selected_algorithms, left_on=['instance_id','algorithm'], right_on=['instance_id','algorithm'])\n",
    "    return df_runs_with_selected_algorithms['PAR10'].mean()\n",
    "\n",
    "evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and compare the performance to our previous selection and to the benchmark values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test\n",
       "average_performance_test_set  7274.185417\n",
       "oracle                        1400.754623\n",
       "single_best(best_avg)         2177.582462\n",
       "single_best(most_often_best)  2953.780503\n",
       "cluster_and_select            1995.379146"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['cluster_and_select'] = evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Results so far\n",
    "\n",
    "We considered two main approaches:\n",
    "- selecting a single best approach and\n",
    "- an unsupervised learning approach (\"cluster-and-select\")\n",
    "and evaluated it using a single train/test split.\n",
    "\n",
    "In addition, we computed two benchmark values (for the same split):\n",
    "- average performance of all algorithms on the test set\n",
    "- best possible performance assuming that we have an \"oracle\" always selecting the best algorithm for each instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Exercise: Play with the Clustering Methods\n",
    "\n",
    "We more or less arbitrarily chose $k$-Means and its parameter (e.g. $k$ = 10).\n",
    "\n",
    "Experiment with the clustering approach and see how this affects the results:\n",
    "- change the number $k$ of clusters used for $k$-Means\n",
    "- change the clustering method. For an overview of possible clustring approaches in scikit-learn, see: \n",
    "  - https://scikit-learn.org/stable/modules/clustering.html (overview of the different methods)\n",
    "  - https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster (functions and paramters to call)\n",
    "\n",
    "Note that in general, you can reuse the the code above and only replace the ``KMeans`` in the following lines:\n",
    "- ``from sklearn.cluster import KMeans``\n",
    "- ``clu = KMeans(n_clusters=10, random_state=1)``\n",
    "\n",
    "with other methods, e.g. with ``FeatureAgglomeration``. Only note that only for some of the approaches, the number of clusters has to be specified in  advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test\n",
       "average_performance_test_set  7274.185417\n",
       "oracle                        1400.754623\n",
       "single_best(best_avg)         2177.582462\n",
       "single_best(most_often_best)  2953.780503\n",
       "cluster_and_select            1995.379146"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How valid are our results?\n",
    "\n",
    "**How valid are these selection results?**\n",
    "- do the results of evaluating a selection using a single train/test split generalize?\n",
    "- is the clustering-based approach superior to the single best\n",
    "  approach in general or only for the given train/test combination?\n",
    "  \n",
    "**Exercise:** Try a different train/test split (by changing the `random_state` parameter in the `train_test_split`function) and compare the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Testing diffferent train/test splits is the central idea of **cross validation**:\n",
    "- try different splits (partitions) into train and test data\n",
    "- and use the average performance of all splits (on all test sets) as performance estimation\n",
    "\n",
    ".. we will consider cross validation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-fold Cross Validation\n",
    "\n",
    "\n",
    "**Key Idea**: Partition the data set into $k$ folds (subsets of equal size)\n",
    "- in each of the $k$ rounds 1 fold is used for testing and the rest of the data for training\n",
    "- model is re-trained in each round\n",
    "- average performance is used to compare different approaches\n",
    "\n",
    "![CrossValidation](img/Cross-Validation-In-Machine-Learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying Cross Validation to Algorithm Selection: Key Steps\n",
    "\n",
    "To evaluate an algorithm selection approach using cross validation:\n",
    "\n",
    "**For each** of the train-test-splits do the following steps:\n",
    "1. **train** the selection approach based on training data\n",
    "2. **select** algorithms for the test set according to the approach\n",
    "3. **evaluate** average performance on test data and **store** it in the results array\n",
    "\n",
    "Finally, take the **average of the results array** to obtain a performance estimate from cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustration: Cross Validation for the Single-Best-Appoach to Algorithm Selection\n",
    "- note: scikit-learn provides functionality for different types of cross validation\n",
    "- here, we use `KFold` cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross validation,  we obtain the following expected performance: 2461.44\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=6,shuffle=True, random_state=11) # we use the parameter \"shuffle=True\" to avoid selecting the subsets according to the order of the instances in the data set\n",
    "results = []\n",
    "for train, test in kf.split(instances):\n",
    "    # split the runs data\n",
    "    df_runs_train_cv, df_runs_test_cv = get_data_train_test(df_runs, instances[train], instances[test]) \n",
    "    \n",
    "    # step 1.: train the selection approach - here: select the best algorithm in the training set\n",
    "    algo = get_algorithm_with_best_average_performance(df_runs_train_cv) \n",
    "    \n",
    "    # step 2.: select the alorithm\n",
    "    df_runs_test_selected_algorithm = df_runs_test_cv[df_runs_test_cv['algorithm']==algo]\n",
    "    \n",
    "    # step 3: evaluate on the test set\n",
    "    results.append(df_runs_test_selected_algorithm['PAR10'].mean())\n",
    "\n",
    "print(f\"Using cross validation,  we obtain the following expected performance: {np.mean(results):.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making the Implementation of Cross Validation More Generic\n",
    "\n",
    "**How can we avoid to write the full loop multiple times for different selection approaches??**\n",
    "- we can try using a function `evaluate_using_cross_validation`\n",
    "\n",
    "**The problem: The three steps in the inner loop are very different for each approach**\n",
    "- possible solution: we give `evaluate_using_cross_validation` a **function** `evaluate_train_test_split` as a parameter\n",
    "\n",
    "The resulting function then looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_using_cross_validation(evaluate_approach_train_test_split, kf):\n",
    "    results = []\n",
    "    for train, test in kf.split(instances):    \n",
    "        results.append(evaluate_approach_train_test_split(instances[train], instances[test]))\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To apply that function for the evaluation for a concrete algorithm selection approach, we need to implement an evaluation function that takes the training and the test instances!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Evaluation Function for the Single-Best Algorithm Selection Approach\n",
    "\n",
    "As an example, let's start with the function `evaluate_train_test_split_single_best`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_train_test_split_single_best (train_instances, test_instances):\n",
    "    \n",
    "    # get data\n",
    "    df_runs_train, df_runs_test = get_data_train_test(df_runs, train_instances, test_instances) \n",
    "    \n",
    "    # step 1.: train the selection approach - here: select the best algorithm in the training set\n",
    "    algo = get_algorithm_with_best_average_performance(df_runs_train) \n",
    "        \n",
    "    # step 2.: select \n",
    "    df_runs_test_selected_algorithm = df_runs_test[df_runs_test['algorithm']==algo]\n",
    "    \n",
    "    # step 3: evaluate on the test set\n",
    "    return df_runs_test_selected_algorithm['PAR10'].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's try it out (compare to the results we got above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2461.442754950495"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_using_cross_validation(evaluate_train_test_split_single_best, kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation Functions for the Benchmarks\n",
    "\n",
    "An implementation for the \"oracle\" benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1383.3625869636965"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_train_test_split_oracle(train_instances, test_instances):\n",
    "    df_best_runs_train, df_best_runs_test = get_data_train_test(df_best_runs, train_instances, test_instances)\n",
    "    \n",
    "    return df_best_runs_test['PAR10'].mean()\n",
    "              \n",
    "evaluate_using_cross_validation(evaluate_train_test_split_oracle,kf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An implementation for the \"average\" benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7161.04845006948"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_train_test_split_average(train_instances, test_instances):\n",
    "    df_runs_train, df_runs_test = get_data_train_test(df_runs, train_instances, test_instances)\n",
    "    \n",
    "    return df_runs_test['PAR10'].mean()\n",
    "\n",
    "evaluate_using_cross_validation(evaluate_train_test_split_average,kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cluster-and-Select\n",
    "- here, we write an evaluation function for the cluster-and-select approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2306.603374092409"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_model = KMeans(n_clusters=10, random_state=0)\n",
    "\n",
    "def evaluate_train_test_split_cluster_and_select(train_instances, test_instances):    \n",
    "    df_runs_train, df_runs_test = get_data_train_test(df_runs, train_instances, test_instances)    \n",
    "    df_instance_features_train, df_instance_features_test =  get_data_train_test(df_instance_features,  train_instances, test_instances)\n",
    "    \n",
    "    # step 1.: train the selection approach - \n",
    "    prd = clustering_model.fit_predict(df_instance_features_train.loc[:, \"f_0\":])  # Create clusters based on training instances and predict the cluster for each of the training instances\n",
    "    df_clustered_instances_train = df_instance_features_train.assign(cluster=prd)[['instance_id','cluster']]  # create a new datafram that contains the instances and their assigned cluster as columns\n",
    "    df_runs_train_with_clusters = pd.merge(df_runs_train, df_clustered_instances_train, left_on='instance_id', right_on='instance_id')   #augment the training set\n",
    "    df_average_performance_clusters = df_runs_train_with_clusters.groupby(['cluster','algorithm'])[\"PAR10\"].mean().reset_index() \n",
    "    df_algorithm_for_cluster_best_avg_performance = df_average_performance_clusters.iloc[df_average_performance_clusters.groupby('cluster')[\"PAR10\"].idxmin()]\n",
    "    df_algorithm_for_cluster_best_avg_performance = df_algorithm_for_cluster_best_avg_performance[['cluster','algorithm']] #only use certain columns\n",
    "    \n",
    "    # step 2.: Select an algorithm for each test set instance\n",
    "    prd = clustering_model.predict(df_instance_features_test.loc[:, \"f_0\":])  # Assign test instances to the created clusters\n",
    "    df_clustered_instances_test = df_instance_features_test.assign(cluster=prd)[['instance_id','cluster']]  # Add the assigned cluster to df_instances as a new column and select only the instance and cluster columns\n",
    "    df_selected_algorithm_test_instances = pd.merge(df_clustered_instances_test, df_algorithm_for_cluster_best_avg_performance, left_on='cluster', right_on='cluster')\n",
    "    \n",
    "    # step 3.: Evaluate the selection\n",
    "    return evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances)\n",
    "\n",
    "evaluate_using_cross_validation(evaluate_train_test_split_cluster_and_select,kf)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All Results in one Data Frame\n",
    "\n",
    "Now, we can put all results in a single data frame along with the original results from last week that were based on a single train-test split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Michael\\miniconda3\\envs\\cords2022\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1332: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['oracle','cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_oracle,kf)\n",
    "df_results.loc['average_performance_test_set','cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_average,kf)      \n",
    "df_results.loc['single_best(best_avg)','cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_single_best,kf)\n",
    "df_results.loc['cluster_and_select','cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_cluster_and_select,kf)      \n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Classification for Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning: Unsupervised vs Supervised Learning\n",
    "\n",
    "**Unsupervised Learning**\n",
    "- given a vector $\\boldsymbol{x}_i$ input data features for each observation / instance $i$\n",
    "- learn something interesting such as relations or clusters \n",
    "\n",
    "**Supervised Learning:**\n",
    "- given a vector $\\boldsymbol{x}_i$  input data features\n",
    "- and a label $y_i$ for each $i$\n",
    "- learn the to predict $y_j$ for an unlabeled feature vector $\\boldsymbol{x}_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning: Classification vs Regression\n",
    "\n",
    "The most important types of supervised learning are:\n",
    "\n",
    "- **classification**: predict which category or type (labels are categorical values or classes)\n",
    "   - our use in algorithm selection: predict the best algorithm \n",
    "   \n",
    "- **regression**: predict how much or how many (labels are numbers)\n",
    "  - our use in algorithm selection: predict the algorithm performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Given a set of *examples* with some given (categorical) *outcomes*, we wish to learn a *model* to predict an outcome given a new example\n",
    "\n",
    "(Classical) **Example**: Categorizing Iris species from flower petal information (e.g. size, length, color,..)\n",
    "\n",
    "![iris](img/iris.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification for Algorithm Configuration\n",
    "\n",
    "One (admittedly naive) approach for using classification for algorithm configuration:\n",
    "\n",
    "**Use instance feature information for predicting the best algorithm**\n",
    "- for training, we need the best algorithm for each instance in the test data set\n",
    "- then, we can use the trained classifier for predicting the best algorithm\n",
    "\n",
    "**In this meeting, we will consider three different classification approaches:**\n",
    "- $k$-nearest neighbor \n",
    "- decision trees\n",
    "- random forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting the Best Algorithm for the MaxSAT Data Set\n",
    "\n",
    "- the MaxSAT data set comes with a file with 37 features per instance\n",
    "- the semantics of the features are unknown to us (there names are f_1, f_2, etc.)\n",
    "- the values are normalized to take values between 0 and 1\n",
    "\n",
    "For training the prediction of the best approach, we need a data frame with **one row per instance** (in the training set) containing\n",
    "- the instance features\n",
    "- the best algorithm\n",
    "    \n",
    "We first create the full data frame for the best runs (including both performance and feature information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>PAR10</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>...</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>f_31</th>\n",
       "      <th>f_32</th>\n",
       "      <th>f_33</th>\n",
       "      <th>f_34</th>\n",
       "      <th>f_35</th>\n",
       "      <th>f_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10tree110p.wcnf</td>\n",
       "      <td>mscg2015a</td>\n",
       "      <td>1.37</td>\n",
       "      <td>6642.0</td>\n",
       "      <td>25589.0</td>\n",
       "      <td>0.00821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00993</td>\n",
       "      <td>0.67185</td>\n",
       "      <td>0.78334</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>0.00133</td>\n",
       "      <td>0.74571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10tree115p.wcnf</td>\n",
       "      <td>mscg2015a</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>16586.0</td>\n",
       "      <td>0.01266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01266</td>\n",
       "      <td>0.65833</td>\n",
       "      <td>0.81647</td>\n",
       "      <td>0.00034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>0.73767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10tree120p.wcnf</td>\n",
       "      <td>mscg2015a</td>\n",
       "      <td>4.88</td>\n",
       "      <td>6708.0</td>\n",
       "      <td>25809.0</td>\n",
       "      <td>0.00814</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00984</td>\n",
       "      <td>0.67635</td>\n",
       "      <td>0.78647</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.74745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10tree125p.wcnf</td>\n",
       "      <td>mscg2015b</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4604.0</td>\n",
       "      <td>16706.0</td>\n",
       "      <td>0.01257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01257</td>\n",
       "      <td>0.66198</td>\n",
       "      <td>0.81839</td>\n",
       "      <td>0.00034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.00192</td>\n",
       "      <td>0.73896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10tree130p.wcnf</td>\n",
       "      <td>mscg2015b</td>\n",
       "      <td>25.09</td>\n",
       "      <td>6744.0</td>\n",
       "      <td>25929.0</td>\n",
       "      <td>0.00810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00980</td>\n",
       "      <td>0.67878</td>\n",
       "      <td>0.78815</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00135</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>0.74839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       instance_id  algorithm  PAR10     f_0      f_1      f_2  f_3  f_4  f_5  \\\n",
       "0  10tree110p.wcnf  mscg2015a   1.37  6642.0  25589.0  0.00821  1.0  0.0  1.0   \n",
       "1  10tree115p.wcnf  mscg2015a   0.90  4564.0  16586.0  0.01266  1.0  0.0  1.0   \n",
       "2  10tree120p.wcnf  mscg2015a   4.88  6708.0  25809.0  0.00814  1.0  0.0  1.0   \n",
       "3  10tree125p.wcnf  mscg2015b   3.20  4604.0  16706.0  0.01257  1.0  0.0  1.0   \n",
       "4  10tree130p.wcnf  mscg2015b  25.09  6744.0  25929.0  0.00810  1.0  0.0  1.0   \n",
       "\n",
       "   f_6  ...  f_27     f_28     f_29     f_30     f_31  f_32     f_33     f_34  \\\n",
       "0  1.0  ...   1.0  0.00993  0.67185  0.78334  0.00024   0.0  0.00004  0.00137   \n",
       "1  1.0  ...   1.0  0.01266  0.65833  0.81647  0.00034   0.0  0.00012  0.00205   \n",
       "2  1.0  ...   1.0  0.00984  0.67635  0.78647  0.00024   0.0  0.00004  0.00136   \n",
       "3  1.0  ...   1.0  0.01257  0.66198  0.81839  0.00034   0.0  0.00012  0.00204   \n",
       "4  1.0  ...   1.0  0.00980  0.67878  0.78815  0.00023   0.0  0.00004  0.00135   \n",
       "\n",
       "      f_35     f_36  \n",
       "0  0.00133  0.74571  \n",
       "1  0.00193  0.73767  \n",
       "2  0.00132  0.74745  \n",
       "3  0.00192  0.73896  \n",
       "4  0.00131  0.74839  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best_runs = pd.merge(df_best_runs, df_instance_features, left_on='instance_id', right_on='instance_id')\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we then split the data frame into a train and a test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_best_runs_train, df_best_runs_test = get_data_train_test(df_best_runs, train_instances, test_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbor Classification\n",
    "\n",
    "**Key Idea:**\n",
    "- store all (or most) of the the labeled training data points\n",
    "- when encountering a new data point, compute the **distance** to the labeled data points \n",
    "- determine the $k$ closest data points (the neighbors)\n",
    "- select the label that occurs most often among these neigbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbor: Illustration\n",
    "\n",
    "**Is it a pear or an apple?**\n",
    "\n",
    "![knn_1](img/example_knn_classification_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbor: Illustration\n",
    "\n",
    "**Is it a pear or an apple?**\n",
    "\n",
    "![knn_2](img/example_knn_classification_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbor: Illustration\n",
    "\n",
    "**Is it a pear or an apple?**\n",
    "\n",
    "![knn_2](img/example_knn_classification_3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbor: Illustration\n",
    "\n",
    "**Is it a pear or an apple?**\n",
    "\n",
    "![knn_2](img/example_knn_classification_4.png)\n",
    "\n",
    "**It is a pear!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbor Classification  for Algorithm Selection\n",
    "\n",
    "**Let's try on our case study data**:\n",
    "\n",
    "First, we use the $k$-Nearest Neighbor Classifier from scikit-learn and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "clf.fit(df_best_runs_train.loc[:, \"f_0\":], df_best_runs_train[\"algorithm\"]) # df_best_runs_train.loc[:, \"f_0\":] is the range of the data frame that contains the feature value\n",
    "\n",
    "prd = clf.predict(df_best_runs_test.loc[:, \"f_0\":])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- a quick assessment of the accuracy is not overly promising:\n",
    "  - (but remember that we have 19 different classes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37185929648241206"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_best_runs_test[\"algorithm\"], prd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-Nearest Neighbor Classification  for Algorithm Selection\n",
    "\n",
    "Let us now predict an algorithm for the test instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prd = clf.predict(df_instance_features_test.loc[:, \"f_0\":]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_selected_algorithm_test_instances  = df_instance_features_test.filter(['instance_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_selected_algorithm_test_instances['algorithm']=prd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3038.238844221106\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    ".. and add it to the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>3038.238844</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374\n",
       "KNeighborsClassifier          3038.238844             NaN"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['KNeighborsClassifier' ,'perf_test'] = evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Evaluate the Performance for the KNN-Classifier with Cross Validation!\n",
    "\n",
    "- write an evaluation function `evaluate_train_test_split_classification_knn` using the code above\n",
    "- check if the results from that function with the single train test split are the same as we computed above\n",
    "- perform a cross validation evaluation using that function\n",
    "- add the results to the missing cell in the results data frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3463.22242029703"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_train_test_split_classification_knn(train_instances, test_instances):\n",
    "    \n",
    "    # split the data frames that we need\n",
    "    df_best_runs_train, df_best_runs_test = get_data_train_test(df_best_runs, train_instances, test_instances)\n",
    "    df_runs_train, df_runs_test =  get_data_train_test(df_runs, train_instances, test_instances)\n",
    "    df_instance_features_train, df_instance_features_test= get_data_train_test(df_instance_features, train_instances, test_instances)\n",
    "    \n",
    "    # step 1: training\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(df_best_runs_train.loc[:, \"f_0\":], df_best_runs_train[\"algorithm\"]) # df_best_runs_train.loc[:, \"f_0\":] is the range of the data frame that contains the feature value\n",
    " \n",
    "    # step 2: selection\n",
    "    prd = clf.predict(df_instance_features_test.loc[:, \"f_0\":])    \n",
    "    df_selected_algorithm_test_instances  = df_instance_features_test.filter(['instance_id'])\n",
    "    df_selected_algorithm_test_instances['algorithm']=prd\n",
    "\n",
    "    #step 3: evaluation\n",
    "    return evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances)\n",
    "\n",
    "evaluate_using_cross_validation(evaluate_train_test_split_classification_knn, kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>3038.238844</td>\n",
       "      <td>3463.222420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374\n",
       "KNeighborsClassifier          3038.238844     3463.222420"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['KNeighborsClassifier' ,'cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_classification_knn,kf) \n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification with Decision Trees\n",
    "\n",
    "Let us now check different classification approaches\n",
    "\n",
    "**Basic Idea:**\n",
    "\n",
    "Create a tree of if-statements to determine the label of a training example.\n",
    "\n",
    "In general, decision trees look as follows:\n",
    "\n",
    "\n",
    "![decision_tree1](img/decision_tree_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification with Decision Trees\n",
    "\n",
    "**Basic Idea:**\n",
    "\n",
    "Create a tree of if-statements to determine the label of a training example.\n",
    "\n",
    "A (partial) decision tree for the iris classification example may look as follows:\n",
    "\n",
    "\n",
    "![decision_tree2](img/decision_tree_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using a Decision Tree Classifier in our Case Study\n",
    "\n",
    "- we use a decision tree from scikit-learn\n",
    "- and replace a single line in the evaluate_train_test_split function:\n",
    "    - `clf = DecisionTreeClassifier(max_depth=10, random_state=0)`\n",
    "- otherwise, the function is identical to the one used for the $k$-nearest neighbour approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def evaluate_train_test_split_classification_decision_tree(train_instances, test_instances):\n",
    "    \n",
    "    #prepare the data\n",
    "    df_instance_features_train, df_instance_features_test = get_data_train_test(df_instance_features, train_instances, test_instances)\n",
    "    df_runs_train, df_runs_test =  get_data_train_test(df_runs, train_instances, test_instances)\n",
    "    df_best_runs_train, df_best_runs_test =  get_data_train_test(df_best_runs, train_instances, test_instances)\n",
    "    \n",
    "    # step 1: Training\n",
    "    clf = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    \n",
    "    clf.fit(df_best_runs_train.loc[:, \"f_0\":], df_best_runs_train[\"algorithm\"])\n",
    "    \n",
    "    # step 2: Selection\n",
    "    prd = clf.predict(df_instance_features_test.loc[:, \"f_0\":])  \n",
    "    \n",
    "    df_selected_algorithm_test_instances = df_instance_features_test.filter(['instance_id'])\n",
    "    \n",
    "    df_selected_algorithm_test_instances['algorithm']=prd\n",
    "\n",
    "    return evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results from using the Decision Tree Classifier\n",
    "- we evaluate the algorithm selection based on predicting the best with the decision tree classifier for the single train-test split and using cross validation\n",
    "- and directly add the results to our results data frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>3038.238844</td>\n",
       "      <td>3463.222420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>2860.636683</td>\n",
       "      <td>2589.799244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374\n",
       "KNeighborsClassifier          3038.238844     3463.222420\n",
       "DecisionTreeClassifier        2860.636683     2589.799244"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['DecisionTreeClassifier' ,'perf_test'] = evaluate_train_test_split_classification_decision_tree(train_instances, test_instances)\n",
    "df_results.loc['DecisionTreeClassifier' ,'cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_classification_decision_tree,kf) \n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "..turns out that the decision tree performs much better than the $k$-NN classifier, but still the clustering-and-select approach is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "**Idea:** If a single learned algorithm is good at making predictions, can we perform even better by simultaneously using multiple algorithms?\n",
    "\n",
    "\n",
    "![ensemble_1](img/ensembles_1.png)\n",
    "\n",
    "**Example**: Three different classication approaches \"vote\" on the correct classification of some input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "**Idea:** If a single learned algorithm is good at making predictions, can we perform even better by simultaneously using multiple algorithms?\n",
    "\n",
    "![ensemble_2](img/ensembles_2.png)\n",
    "\n",
    "**Example**: Three different classication approaches \"vote\" on the correct classification of some input data\n",
    "\n",
    "**$\\rightarrow$ The majority vote wins!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensembles with Bagging\n",
    "\n",
    "**Some classifiers exibit a high degree of variance:**\n",
    "- small changes in the data lead to vastly different models\n",
    "\n",
    "This behavior can be overcome with **Bagging**: **B**ootstrap **Agg**regation.\n",
    "\n",
    "The idea of bagging intuitively works as follows:\n",
    "- bootstrap (sample with replacement) from the training data multiple times\n",
    "- train a classifier for each of the bootstrapped training sets\n",
    "- aggregate the predicitions form all classifiers by\n",
    "    - majority voting (for classication problems) \n",
    "    - averaging (for regression problems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests: Bagging with Decision Trees\n",
    "\n",
    "Since decision trees have a high variance, bagging can be used for creating stable predictions.\n",
    "\n",
    "![bagging](img/bagging.png)\n",
    "\n",
    "\n",
    "As we use multiple trees, the resuling model is called a **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using a Random Forest Classifier in our Case Study\n",
    "\n",
    "- we use a random forest from scikit-learn\n",
    "- and replace a single line in the evaluate_train_test_split function:\n",
    "    - `RandomForestClassifier(max_depth=10, random_state=0)`\n",
    "- otherwise, the evaluation function is identical to the one used for the $k$-nearest neighbour approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evaluate_train_test_split_classification_random_forest(train_instances, test_instances):\n",
    "    \n",
    "    #prepare the data\n",
    "    df_instance_features_train, df_instance_features_test = get_data_train_test(df_instance_features, train_instances, test_instances)\n",
    "    df_runs_train, df_runs_test =  get_data_train_test(df_runs, train_instances, test_instances)\n",
    "    df_best_runs_train, df_best_runs_test =  get_data_train_test(df_best_runs, train_instances, test_instances)\n",
    "    \n",
    "    # step 1: Training\n",
    "    clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "    \n",
    "    clf.fit(df_best_runs_train.loc[:, \"f_0\":], df_best_runs_train[\"algorithm\"])\n",
    "    \n",
    "    # step 2: Selection\n",
    "    prd = clf.predict(df_instance_features_test.loc[:, \"f_0\":])  \n",
    "    \n",
    "    df_selected_algorithm_test_instances = df_instance_features_test.filter(['instance_id'])\n",
    "    \n",
    "    df_selected_algorithm_test_instances['algorithm']=prd\n",
    "\n",
    "    return evaluate_selected_algorithms(df_runs_test, df_selected_algorithm_test_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results from using the Random Forest Classifier\n",
    "\n",
    "- we evaluate the algorithm selection based on predicting the best with the decision tree classifier for the single train-test split and using cross validation\n",
    "- and directly add the results to our results data frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>3038.238844</td>\n",
       "      <td>3463.222420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>2860.636683</td>\n",
       "      <td>2589.799244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>2578.188995</td>\n",
       "      <td>2343.732815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374\n",
       "KNeighborsClassifier          3038.238844     3463.222420\n",
       "DecisionTreeClassifier        2860.636683     2589.799244\n",
       "RandomForestClassifier        2578.188995     2343.732815"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['RandomForestClassifier' ,'perf_test'] = evaluate_train_test_split_classification_random_forest(train_instances, test_instances)\n",
    "df_results.loc['RandomForestClassifier' ,'cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_classification_random_forest,kf) \n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "..turns out that the random forest yields the best results among the classification-based approaches, but still the clustering-and-select approach is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Regression for Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning: Classification vs Regression\n",
    "\n",
    "The most important types of supervised learning are:\n",
    "\n",
    "- **classification**: predict which category or type (labels are categorical values or classes)\n",
    "   - our use in algorithm selection: predict the best algorithm \n",
    "   \n",
    "- **regression**: predict how much or how many (labels are numbers)\n",
    "  - our use in algorithm selection: predict the algorihtm performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning: Regression\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Given a set of *examples* with some given (categorical) *outcomes*, we wish to learn a *model* to predict an outcome given a new example\n",
    "\n",
    "**Example**: If I water my trees with $X$ liter of water in May, how tall will they grow?\n",
    "\n",
    "![reg](img/regression_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustrating Linear Regression\n",
    "\n",
    "Consider our tree growth example again..\n",
    "![reg](img/regression_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustrating Linear Regression\n",
    "\n",
    "Consider our tree growth example again..\n",
    "![reg](img/regression_2.png)\n",
    "\n",
    "- **Assumption:** Linear relation between water and height\n",
    "- **Note:** Missing data for 2 and 4 l of water\n",
    "- **Prediction:** 4 l of water yields a heigt between 3 and 5 metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression for Algorithm Selection\n",
    "\n",
    "**Key Idea: Predict Runtime and Select Algorithm**\n",
    "- for each algorithm train a regression model that used instance\n",
    "  features and PAR10 labels to predict runtime (PAR10) performance\n",
    "- when encountering a new instance, chose the algorithm with the best predicted performance\n",
    "\n",
    "**Observe:**\n",
    "- instead of fitting a single model, we need to fit **one model for each algorithm**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting the Best Algorithm for the MaxSAT Data Set\n",
    "\n",
    "- the MaxSAT data set comes with a file with 37 features per instance\n",
    "- the semantics of the features are unknown to us (there names are f_1, f_2, etc.)\n",
    "- the values are normalized to take values between 0 and 1\n",
    "\n",
    "For training a regressor that predicts the PAR10 performance of an algorithm of an instance, we need \n",
    "- a data frame with **one row per combination of instance and algorithm** (in the training set) containing\n",
    "- the instance \n",
    "- the instance features\n",
    "- the algorithm\n",
    "- the PAR10 value\n",
    "    \n",
    "We first create the full data frame including both performance and feature information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>PAR10</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>...</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>f_31</th>\n",
       "      <th>f_32</th>\n",
       "      <th>f_33</th>\n",
       "      <th>f_34</th>\n",
       "      <th>f_35</th>\n",
       "      <th>f_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>CCEHC2akms</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>15008.0</td>\n",
       "      <td>80288.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>ahms-1.70</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>15008.0</td>\n",
       "      <td>80288.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>LMHS-2016</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>15008.0</td>\n",
       "      <td>80288.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>Optiriss6</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>15008.0</td>\n",
       "      <td>80288.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mul_8_11.wcnf</td>\n",
       "      <td>WPM3-2015-co</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>15008.0</td>\n",
       "      <td>80288.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.18693</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     instance_id     algorithm    PAR10      f_0      f_1      f_2  f_3  f_4  \\\n",
       "0  mul_8_11.wcnf    CCEHC2akms  18000.0  15008.0  80288.0  0.18693  1.0  0.0   \n",
       "1  mul_8_11.wcnf     ahms-1.70  18000.0  15008.0  80288.0  0.18693  1.0  0.0   \n",
       "2  mul_8_11.wcnf     LMHS-2016  18000.0  15008.0  80288.0  0.18693  1.0  0.0   \n",
       "3  mul_8_11.wcnf     Optiriss6  18000.0  15008.0  80288.0  0.18693  1.0  0.0   \n",
       "4  mul_8_11.wcnf  WPM3-2015-co  18000.0  15008.0  80288.0  0.18693  1.0  0.0   \n",
       "\n",
       "   f_5  f_6  ...  f_27     f_28     f_29     f_30     f_31  f_32     f_33  \\\n",
       "0  1.0  1.0  ...   1.0  0.18693  0.18693  0.18693  0.00001   0.0  0.00001   \n",
       "1  1.0  1.0  ...   1.0  0.18693  0.18693  0.18693  0.00001   0.0  0.00001   \n",
       "2  1.0  1.0  ...   1.0  0.18693  0.18693  0.18693  0.00001   0.0  0.00001   \n",
       "3  1.0  1.0  ...   1.0  0.18693  0.18693  0.18693  0.00001   0.0  0.00001   \n",
       "4  1.0  1.0  ...   1.0  0.18693  0.18693  0.18693  0.00001   0.0  0.00001   \n",
       "\n",
       "      f_34  f_35     f_36  \n",
       "0  0.00001   0.0  0.18693  \n",
       "1  0.00001   0.0  0.18693  \n",
       "2  0.00001   0.0  0.18693  \n",
       "3  0.00001   0.0  0.18693  \n",
       "4  0.00001   0.0  0.18693  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_runs = pd.merge(df_runs, df_instance_features, left_on='instance_id', right_on='instance_id')\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Split this data frame into a train and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_runs_train, df_runs_test =  get_data_train_test(df_runs, train_instances, test_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Linear Regression for Algorithm Selection: Case Study\n",
    "\n",
    "We will use the linear regression from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "First, we will create one prediction model per algorithm.\n",
    "\n",
    "For each algorithm, we:\n",
    "- filter the training data set to contain only runs with this algorithm\n",
    "- fit a regression model using instance features as features and PAR10 values as labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "runtime_prediction_models = {} # a dictionary / map where we will store the fitted model for each algorithm\n",
    "\n",
    "# for alg in algorithms:\n",
    "for alg in algorithms: # by using tdqm, we can monitor the progress of the loop\n",
    "\n",
    "    reg = LinearRegression()\n",
    "\n",
    "    df_runs_train_alg = df_runs_train[df_runs_train[\"algorithm\"] == alg]  # only consider runs with the algorithm under consideration\n",
    "\n",
    "    reg.fit(df_runs_train_alg.loc[:, \"f_0\":], df_runs_train_alg[\"PAR10\"])  # Train a model for algorithm alg to predict the PAR10 value on an instance based on the instance features \n",
    "    runtime_prediction_models[alg] = reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Linear Regression for Algorithm Selection: Case Study\n",
    "\n",
    "Then, we perform the selection of algorithm in the test set as follows:\n",
    "- predict the performance for each algorithm in each instance in the test set \n",
    "- for each instance in the test set, select the algorithm with the best predicted performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_set_predictions=[]\n",
    "\n",
    "# create a list of data frames (one per algorithm) containing the PAR10 predictions for the test instances\n",
    "for alg, reg in runtime_prediction_models.items():\n",
    "    df_instance_with_prediction_test = df_instance_features_test.assign(algorithm=alg, PAR10_prediction = reg.predict(df_instance_features_test.loc[:, \"f_0\":]))\n",
    "    test_set_predictions.append(df_instance_with_prediction_test)\n",
    "\n",
    "# combine the dataframes into a single one\n",
    "df_instance_with_prediction_test = pd.concat(test_set_predictions, join=\"inner\").reset_index(drop=True)  # Concatenate the results of all alg\n",
    "\n",
    "# for each instance, select the algorithm with the best predicted performance\n",
    "row_idx_for_best_predicted_algorithms = df_instance_with_prediction_test.groupby(\"instance_id\")[\"PAR10_prediction\"].idxmin()  # For each instance, get the row id of the entry with the lowest PAR10 prediction\n",
    "df_selected_algorithms = df_instance_with_prediction_test.iloc[row_idx_for_best_predicted_algorithms ]  # We use that to filter only those rows\n",
    "\n",
    "#df_selected_algorithms[[\"instance_id\",\"algorithm\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let us evaluate the average performance of the selection on the test instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991.6450753768845\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>3038.238844</td>\n",
       "      <td>3463.222420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>2860.636683</td>\n",
       "      <td>2589.799244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>2578.188995</td>\n",
       "      <td>2343.732815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>1991.645075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374\n",
       "KNeighborsClassifier          3038.238844     3463.222420\n",
       "DecisionTreeClassifier        2860.636683     2589.799244\n",
       "RandomForestClassifier        2578.188995     2343.732815\n",
       "LinearRegression              1991.645075             NaN"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(evaluate_selected_algorithms(df_runs_test, df_selected_algorithms))\n",
    "df_results.loc[type(reg).__name__ ,'perf_test'] = np.mean(evaluate_selected_algorithms(df_runs_test, df_selected_algorithms))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Evaluate the Performance for the Linear Regression Approach with Cross Validation!\n",
    "\n",
    "- write an evaluation function `evaluate_train_test_split_regression_linear` using the code above\n",
    "- check if the results from that function with the single train test split are the same as we computed above\n",
    "- perform a cross validation evaluation using that function\n",
    "- add the results to the missing cell in the results data frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression Trees\n",
    "\n",
    "Decision Trees can not only be used for classification, but also for regression:\n",
    "- same idea as for classication\n",
    "- instead of classes, leaves represent continous values\n",
    "- example: predicting the level of pollution\n",
    "\n",
    "\n",
    "![reg](img/regression_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case Study: Regression Trees for Algorithm Selection: Evaluation Function\n",
    "\n",
    "The evaluation function looks *almost* the same as for linear regression,\n",
    "- we use replace the line in which we define the regression model:  `reg = DecisionTreeRegressor(max_depth=10, random_state=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def evaluate_train_test_split_regression_tree(train_instances, test_instances):\n",
    "    #prepare the data\n",
    "    df_instance_features_train, df_instance_features_test = get_data_train_test(df_instance_features, train_instances, test_instances)\n",
    "    df_runs_train, df_runs_test =  get_data_train_test(df_runs, train_instances, test_instances)\n",
    "  \n",
    "    \n",
    "    # step 1: training    \n",
    "    runtime_prediction_models = {} # a dictionary / map where we will store the fitted model for each algorithm\n",
    "\n",
    "    # for alg in algorithms:\n",
    "    for alg in algorithms: # by using tdqm, we can monitor the progress of the loop\n",
    "\n",
    "        reg = DecisionTreeRegressor(max_depth=10, random_state=0)\n",
    "\n",
    "        df_runs_train_alg = df_runs_train[df_runs_train[\"algorithm\"] == alg]  # only consider runs with the algorithm under consideration\n",
    "        reg.fit(df_runs_train_alg.loc[:, \"f_0\":], df_runs_train_alg[\"PAR10\"])  # Train a model for algorithm alg to predict the PAR10 value on an instance based on the instance features \n",
    "        runtime_prediction_models[alg] = reg\n",
    "\n",
    "    # step 2\n",
    "    test_set_predictions=[]\n",
    "\n",
    "    for alg, reg in runtime_prediction_models.items():\n",
    "        df_instance_with_prediction_test = df_instance_features_test.assign(algorithm=alg, PAR10_prediction = reg.predict(df_instance_features_test.loc[:, \"f_0\":]))\n",
    "        test_set_predictions.append(df_instance_with_prediction_test)\n",
    "\n",
    "    df_instance_with_prediction_test = pd.concat(test_set_predictions, join=\"inner\").reset_index(drop=True)  # Concatenate the results of all alg\n",
    "\n",
    "    row_idx_for_best_predicted_algorithms = df_instance_with_prediction_test.groupby(\"instance_id\")[\"PAR10_prediction\"].idxmin()  # For each instance, get the row id of the entry with the lowest PAR10 prediction\n",
    "\n",
    "    df_selected_algorithms = df_instance_with_prediction_test.iloc[row_idx_for_best_predicted_algorithms ]  # We use that to filter only those rows\n",
    "\n",
    "    df_selected_algorithms[[\"instance_id\",\"algorithm\"]]\n",
    "    \n",
    "    # step 3\n",
    "    return evaluate_selected_algorithms(df_runs_test, df_selected_algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case Study: Regression Trees for Algorithm Selection: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>3038.238844</td>\n",
       "      <td>3463.222420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>2860.636683</td>\n",
       "      <td>2589.799244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>2578.188995</td>\n",
       "      <td>2343.732815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>1991.645075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>2604.766884</td>\n",
       "      <td>2381.047806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374\n",
       "KNeighborsClassifier          3038.238844     3463.222420\n",
       "DecisionTreeClassifier        2860.636683     2589.799244\n",
       "RandomForestClassifier        2578.188995     2343.732815\n",
       "LinearRegression              1991.645075             NaN\n",
       "DecisionTreeRegressor         2604.766884     2381.047806"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['DecisionTreeRegressor' ,'perf_test'] = evaluate_train_test_split_regression_tree(train_instances, test_instances)\n",
    "df_results.loc['DecisionTreeRegressor' ,'cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_regression_tree, kf)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "..(much) worse than linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging for Regression\n",
    "\n",
    "![reg](img/regression_bagging.png)\n",
    "- grey lines: predicion based on bootstrapped regression models\n",
    "- red line: average of the rey lines (result from aggregation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests for Regression\n",
    "\n",
    "**We can also use Random Forests for Regression:**\n",
    "- applying bagging to multiple regression trees\n",
    "- create multiple regression trees for subset of the training data\n",
    "- average the results of the trees to obtain a robust prediction\n",
    "\n",
    "\n",
    "$\\rightarrow$ same idea as for random forests for classification\n",
    "\n",
    "**For use in algorithm selection**\n",
    "- we simply use scikit-learn's `RandomForestRegressor` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case Study: Regression Trees for Algorithm Selection: Evaluation Function\n",
    "\n",
    "The evaluation function looks *almost* the same as for linear regression,\n",
    "- we use replace the line in which we define the regression model:  `reg = RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def evaluate_train_test_split_regression_forest(train_instances, test_instances):\n",
    "    #prepare the data\n",
    "    df_instance_features_train, df_instance_features_test = get_data_train_test(df_instance_features, train_instances, test_instances)\n",
    "    df_runs_train, df_runs_test =  get_data_train_test(df_runs, train_instances, test_instances)\n",
    "  \n",
    "    \n",
    "    # step 1: training    \n",
    "    runtime_prediction_models = {} # a dictionary / map where we will store the fitted model for each algorithm\n",
    "\n",
    "    # for alg in algorithms:\n",
    "    for alg in algorithms: # by using tdqm, we can monitor the progress of the loop\n",
    "\n",
    "        reg = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "\n",
    "        df_runs_train_alg = df_runs_train[df_runs_train[\"algorithm\"] == alg]  # only consider runs with the algorithm under consideration\n",
    "        reg.fit(df_runs_train_alg.loc[:, \"f_0\":], df_runs_train_alg[\"PAR10\"])  # Train a model for algorithm alg to predict the PAR10 value on an instance based on the instance features \n",
    "        runtime_prediction_models[alg] = reg\n",
    "\n",
    "    # step 2\n",
    "    test_set_predictions=[]\n",
    "\n",
    "    for alg, reg in runtime_prediction_models.items():\n",
    "        df_instance_with_prediction_test = df_instance_features_test.assign(algorithm=alg, PAR10_prediction = reg.predict(df_instance_features_test.loc[:, \"f_0\":]))\n",
    "        test_set_predictions.append(df_instance_with_prediction_test)\n",
    "\n",
    "    df_instance_with_prediction_test = pd.concat(test_set_predictions, join=\"inner\").reset_index(drop=True)  # Concatenate the results of all alg\n",
    "\n",
    "    row_idx_for_best_predicted_algorithms = df_instance_with_prediction_test.groupby(\"instance_id\")[\"PAR10_prediction\"].idxmin()  # For each instance, get the row id of the entry with the lowest PAR10 prediction\n",
    "\n",
    "    df_selected_algorithms = df_instance_with_prediction_test.iloc[row_idx_for_best_predicted_algorithms ]  # We use that to filter only those rows\n",
    "\n",
    "    df_selected_algorithms[[\"instance_id\",\"algorithm\"]]\n",
    "    \n",
    "    # step 3\n",
    "    return evaluate_selected_algorithms(df_runs_test, df_selected_algorithms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression Forests for Algorithm Selection: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_test</th>\n",
       "      <th>cross_val_perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>average_performance_test_set</th>\n",
       "      <td>7274.185417</td>\n",
       "      <td>7161.048450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oracle</th>\n",
       "      <td>1400.754623</td>\n",
       "      <td>1383.362587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(best_avg)</th>\n",
       "      <td>2177.582462</td>\n",
       "      <td>2461.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single_best(most_often_best)</th>\n",
       "      <td>2953.780503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_and_select</th>\n",
       "      <td>1995.379146</td>\n",
       "      <td>2306.603374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>3038.238844</td>\n",
       "      <td>3463.222420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>2860.636683</td>\n",
       "      <td>2589.799244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>2578.188995</td>\n",
       "      <td>2343.732815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>1991.645075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>2604.766884</td>\n",
       "      <td>2381.047806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>1892.741307</td>\n",
       "      <td>1843.295538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                perf_test  cross_val_perf\n",
       "average_performance_test_set  7274.185417     7161.048450\n",
       "oracle                        1400.754623     1383.362587\n",
       "single_best(best_avg)         2177.582462     2461.442755\n",
       "single_best(most_often_best)  2953.780503             NaN\n",
       "cluster_and_select            1995.379146     2306.603374\n",
       "KNeighborsClassifier          3038.238844     3463.222420\n",
       "DecisionTreeClassifier        2860.636683     2589.799244\n",
       "RandomForestClassifier        2578.188995     2343.732815\n",
       "LinearRegression              1991.645075             NaN\n",
       "DecisionTreeRegressor         2604.766884     2381.047806\n",
       "RandomForestRegressor         1892.741307     1843.295538"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.loc['RandomForestRegressor' ,'perf_test'] = evaluate_train_test_split_regression_forest(train_instances, test_instances)\n",
    "df_results.loc['RandomForestRegressor' ,'cross_val_perf'] = evaluate_using_cross_validation(evaluate_train_test_split_regression_forest, kf)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "..best performance so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "**This week, we**\n",
    "- started with the second part of the course\n",
    "- introduced the problem of algorithm selection\n",
    "- introduced a case study data set for motivating and implementing our approaches\n",
    "- started using feature information for **instance-specific** algorithm selection\n",
    "- discussed how to use cross validation for obtaining more robust selection results\n",
    "- reviewed different approaches for **unsupervised learning** and **supervised learning** (classification an regression)\n",
    "- learned how to use these approaches for algorithm selection\n",
    "\n",
    "**Next time, we**\n",
    "- will learn how to use machine learning for **algorithm configuration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cords2022]",
   "language": "python",
   "name": "conda-env-cords2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
